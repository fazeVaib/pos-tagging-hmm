{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Vocabulary Creation\n",
    "\n",
    "Here, we're filtering out words that appears only once in train dataset and replacing with unk tag. Then we're removing unk from middle of vocabulary and concatinating it on top. Rest all vocb is sorted by number of occurances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_occ(row):\n",
    "    if row.occur < 2:\n",
    "        return \"<unk>\"\n",
    "    return row.word\n",
    "\n",
    "df_train = pd.read_csv(\"./data/train\", sep = \"\\t\", names = ['idx_sent', 'word', 'tag'])\n",
    "df_train['occur'] = df_train.groupby('word')[\"word\"].transform('size')\n",
    "df_train[\"word\"] = df_train.apply(lambda row: filter_by_occ(row), axis=1)\n",
    "\n",
    "vocab = df_train.word.value_counts().rename_axis(\"unique_words\").reset_index(name='occur')\n",
    "df_unk = vocab[vocab['unique_words'] == \"<unk>\"]\n",
    "index = vocab[vocab.unique_words == \"<unk>\"].index\n",
    "vocab = vocab.drop(index)\n",
    "vocab = pd.concat([df_unk, vocab]).reset_index(drop = True)\n",
    "vocab['idx'] = vocab.index + 1\n",
    "vocab = vocab[[\"unique_words\", \"idx\", \"occur\"]]\n",
    "np.savetxt('./vocab.txt', vocab, fmt = \"%s\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for unknown words:  1\n",
      "Total size of Vocabulary:  23183\n",
      "Occurances of <unk>:  20011\n"
     ]
    }
   ],
   "source": [
    "print(\"Threshold for unknown words: \", 1)\n",
    "print(\"Total size of Vocabulary: \", vocab.shape[0])\n",
    "print(\"Occurances of <unk>: \", int(vocab[vocab.unique_words == \"<unk>\"].occur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dictionaries for tags and reformatting sentences in the form of 2D list from the train dataframe. The list inside the list would contain tuples with word and its corresponding tag in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38218\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "def df_to_list(df_train):\n",
    "    train = []\n",
    "    tmp = []\n",
    "    for row in df_train.values:\n",
    "        if row[0] == 1:\n",
    "            if len(tmp) != 0:\n",
    "                train.append(tmp)\n",
    "                tmp = []\n",
    "        tmp.append((row[1], row[2]))\n",
    "\n",
    "    train.append(tmp)\n",
    "    return train\n",
    "\n",
    "sentences_train = df_to_list(df_train)\n",
    "print(len(sentences_train))\n",
    "\n",
    "df_tag = df_train.tag.value_counts().rename_axis('tag').reset_index(name = 'count')\n",
    "tag_dict = dict(df_tag.values)\n",
    "tags = df_tag.tag.tolist()\n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: HMM Learning\n",
    "\n",
    "Below two functions are used in creating 2D matrices of transition probability and emission probability repectively.\n",
    "They use the formula mentioned in the question itself.\n",
    "\n",
    "To improve the results, I have add some minor probability to zero probability cases so that those caes can be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_mat(sentences, tags):\n",
    "    tag_occur = dict()\n",
    "    trans_mat = np.zeros((len(tags),len(tags)))\n",
    "\n",
    "    for tag in range(len(tags)):\n",
    "        tag_occur[tag] = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            tag_occur[tags.index(sentence[i][1])] += 1\n",
    "            if i == 0: \n",
    "                continue\n",
    "\n",
    "            trans_mat[tags.index(sentence[i - 1][1])][tags.index(sentence[i][1])] += 1\n",
    "    \n",
    "    for i in range(trans_mat.shape[0]):\n",
    "        for j in range(trans_mat.shape[1]):\n",
    "\n",
    "            # removing cases that have zero probability and putting minimum probability\n",
    "            if trans_mat[i][j] == 0: \n",
    "                trans_mat[i][j] = 1e-10\n",
    "            else: \n",
    "                trans_mat[i][j] /= tag_occur[i]\n",
    "\n",
    "    return trans_mat\n",
    "\n",
    "\n",
    "def get_emission_mat(tags, vocab, sentences):\n",
    "    tag_occur = dict()\n",
    "    em_mat = np.zeros((len(tags), len(vocab)))\n",
    "\n",
    "    for tag in range(len(tags)):\n",
    "        tag_occur[tag] = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word, pos in sentence:\n",
    "            tag_occur[tags.index(pos)] +=1\n",
    "            em_mat[tags.index(pos)][vocab.index(word)] += 1\n",
    "\n",
    "    for i in range(em_mat.shape[0]):\n",
    "        for j in range(em_mat.shape[1]):\n",
    "\n",
    "            # removing cases that have zero probability and putting minimum probability\n",
    "            if em_mat[i][j] == 0: \n",
    "                em_mat[i][j] = 1e-10\n",
    "            else: \n",
    "                em_mat[i][j] /= tag_occur[i]\n",
    "\n",
    "    return em_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dictionaries to get transition probabilty from one tag to other and emision probability for a word and tag.\n",
    "\n",
    "they keys are the comma separated values.\n",
    "For transition, it is tags sepearted by comma between parenthesis.\n",
    "For emission, it is tag and wor separated by comma between parentheis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_prob(tags, trans_mat):\n",
    "\n",
    "    tag_dict = dict()\n",
    "    for i, tag in enumerate(tags):\n",
    "        tag_dict[i] = tag\n",
    "\n",
    "    trans_prob = dict()\n",
    "    \n",
    "    for i in range(trans_mat.shape[0]):\n",
    "        for j in range(trans_mat.shape[1]):\n",
    "            trans_prob['(' + tag_dict[i] + ',' + tag_dict[j] + ')'] = trans_mat[i][j]\n",
    "\n",
    "    return trans_prob\n",
    "\n",
    "\n",
    "def get_emission_prob(tags, vocab, em_mat):\n",
    "\n",
    "    tag_dict = dict()\n",
    "    for i, tag in enumerate(tags):\n",
    "        tag_dict[i] = tag\n",
    "\n",
    "    em_prob = dict()\n",
    "    for i in range(em_mat.shape[0]):\n",
    "        for j in range(em_mat.shape[1]):\n",
    "            em_prob['(' + tag_dict[i] + ',' + vocab[j] + ')'] = em_mat[i][j]\n",
    "\n",
    "    return em_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function is used to calculate initial probability T(s1) for the first word tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inital_prob(df, tags):\n",
    "    \n",
    "    tags_start_occ = dict()\n",
    "    for tag in tags:\n",
    "        tags_start_occ[tag] = 0\n",
    "    \n",
    "    total_start_sum = 0\n",
    "    for row in df.itertuples():\n",
    "        if(row[1] == 1):\n",
    "            tags_start_occ[row[3]]+=1\n",
    "            total_start_sum += 1\n",
    "    \n",
    "    prior_prob = {}\n",
    "    for key in tags_start_occ:\n",
    "        prior_prob[key] = tags_start_occ[key] / total_start_sum\n",
    "    \n",
    "    return prior_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating all probabilities and storing them in json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = vocab.unique_words.tolist()\n",
    "\n",
    "init_prob = get_inital_prob(df_train, tags)\n",
    "trans_mat = get_transition_mat(sentences_train, tags)\n",
    "em_mat = get_emission_mat(tags, vocab_list, sentences_train)\n",
    "trans_prob = get_transition_prob(tags, trans_mat)\n",
    "em_prob = get_emission_prob(tags, vocab_list, em_mat)\n",
    "\n",
    "with open('hmm.json', 'w') as f:\n",
    "    json.dump({\"transition\": trans_prob, \"emission\": em_prob}, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Parameters: 2070\n",
      "Emission Parameters: 1043235\n"
     ]
    }
   ],
   "source": [
    "print('Transition Parameters: {}'.format(len(trans_prob) + len(init_prob)))\n",
    "print('Emission Parameters: {}'.format(len(em_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: HMM using Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5527\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing validation data in the required format\n",
    "df_valid = pd.read_csv(\"./data/dev\", sep = \"\\t\", names = ['idx_sent', 'word', 'tag'])\n",
    "df_valid['occur'] = df_valid.groupby('word')[\"word\"].transform('size')\n",
    "\n",
    "sentences_valid = df_to_list(df_valid)\n",
    "print(len(sentences_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function is used to calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pred_op, orig_op):\n",
    "    count = 0\n",
    "    corr_count = 0\n",
    "    for i in range(len(orig_op)):\n",
    "        for j in range(len(orig_op[i])):\n",
    "\n",
    "            if(pred_op[i][j] == orig_op[i][j][1]):\n",
    "                corr_count += 1\n",
    "            count +=1\n",
    "    \n",
    "    return corr_count / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy Decoding fxn: In greedy decoding for every change in states we are calculating the score and storing tag which is giving maximum score and using it as previous state.\n",
    "best_score and state_score are used to keep track of the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for greedy decoding for validation dataset: 0.94\n"
     ]
    }
   ],
   "source": [
    "def greedy_decoding(trans_prob, em_prob, init_prob, sentences_valid, tags):\n",
    "    res = []\n",
    "\n",
    "    for sentence in sentences_valid:\n",
    "        \n",
    "        prev_tag = None\n",
    "        seq = []\n",
    "        for i in range(len(sentence)):\n",
    "            \n",
    "            best_score = -1\n",
    "            for j in range(len(tags)):\n",
    "                \n",
    "                state_score = 1\n",
    "                if i == 0:\n",
    "                    state_score *= init_prob[tags[j]]\n",
    "                else:\n",
    "                    if str(\"(\" + prev_tag  + \",\" + tags[j] + \")\") in trans_prob:\n",
    "                        state_score *= trans_prob[\"(\" + prev_tag  + \",\" + tags[j] + \")\"]\n",
    "                \n",
    "                if str(\"(\" + tags[j] + \",\" + sentence[i][0] + \")\") in em_prob:\n",
    "                    state_score *= em_prob[\"(\" + tags[j] + \",\" + sentence[i][0] + \")\"]\n",
    "                else:\n",
    "                    state_score *= em_prob[\"(\" + tags[j] + \",\" + \"<unk>\" + \")\"]\n",
    "                \n",
    "                if(state_score > best_score):\n",
    "                    best_score = state_score\n",
    "                    highest_prob_tag = tags[j]\n",
    "                    \n",
    "            prev_tag = highest_prob_tag\n",
    "            seq.append(prev_tag)\n",
    "        \n",
    "        res.append(seq)\n",
    "    \n",
    "    return res\n",
    "\n",
    "greedy_valid_op = greedy_decoding(trans_prob, em_prob, init_prob, sentences_valid, tags)\n",
    "print(\"Accuracy for greedy decoding for validation dataset: {:.2f}\".format(get_accuracy(greedy_valid_op, sentences_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating outfile\n",
    "def output_file(test_inputs, test_outputs, filename):\n",
    "    res = []\n",
    "    for i in range(len(test_inputs)):\n",
    "        s = []\n",
    "        for j in range(len(test_inputs[i])):\n",
    "            s.append((str(j+1), test_inputs[i][j], test_outputs[i][j]))\n",
    "        res.append(s)\n",
    "    \n",
    "    with open(filename + \".out\", 'w') as f:\n",
    "        for ele in res:\n",
    "            f.write(\"\\n\".join([str(item[0]) + \"\\t\" + item[1] + \"\\t\" + item[2] for item in ele]))\n",
    "            f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5462\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"./data/test\", sep = \"\\t\", names = ['idx_sent', 'word'])\n",
    "df_test['occur'] = df_test.groupby('word')[\"word\"].transform('size')\n",
    "\n",
    "sentences_test = []\n",
    "sentence = []\n",
    "first = 1\n",
    "for row in df_test.itertuples():\n",
    "    if(row.idx_sent == 1 and first == 0):\n",
    "        sentences_test.append(sentence)\n",
    "        sentence = []\n",
    "    first = 0\n",
    "    sentence.append(row.word)\n",
    "sentences_test.append(sentence)\n",
    "print(len(sentences_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating and storing result of greedy ddecoding on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_test_op = greedy_decoding(trans_prob, em_prob, init_prob, sentences_test, tags)\n",
    "output_file(sentences_test, greedy_test_op, \"greedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Viterbi Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoding(trans_prob, em_prob, prior_prob, s, tags):\n",
    "\n",
    "    n = len(tags)\n",
    "    viterbi_list = []\n",
    "    cache = {}\n",
    "\n",
    "    # cache is a dictionary storing all indides of pos and \"pos\" as a key and value as score or cumulative probability\n",
    "    # Dictionary will only make update when for any state we find that a transition for one tag to another is better than other\n",
    "    # transition mapping.\n",
    "    \n",
    "    for si in tags:\n",
    "        if str(\"(\" + si + \",\" + s[0][0] + \")\") in em_prob:\n",
    "            viterbi_list.append(prior_prob[si] * em_prob[\"(\" + si + \",\" + s[0][0] + \")\"])\n",
    "        \n",
    "        else:\n",
    "            viterbi_list.append(prior_prob[si] * em_prob[\"(\" + si + \",\" + \"<unk>\" + \")\"])\n",
    "\n",
    "    for i, l in enumerate(s):\n",
    "        \n",
    "        word = l[0]\n",
    "        if i == 0: \n",
    "            continue\n",
    "        \n",
    "        temp_list = [None] * n\n",
    "        for j, tag in enumerate(tags):\n",
    "            score = -1\n",
    "            val = 1\n",
    "            \n",
    "            for k, prob in enumerate(viterbi_list):\n",
    "                if str(\"(\" + tags[k] + \",\" + tag + \")\") in trans_prob and str(\"(\" + tag + \",\" + word + \")\") in em_prob:\n",
    "                    val = prob * trans_prob[\"(\" + tags[k] + \",\" + tag + \")\"] * em_prob[\"(\" + tag + \",\" + word + \")\"]\n",
    "                \n",
    "                else:\n",
    "                   val = prob * trans_prob[\"(\" + tags[k] + \",\" + tag + \")\"] * em_prob[\"(\" + tag + \",\" + \"<unk>\" + \")\"]\n",
    "                \n",
    "                if(score < val):\n",
    "                    score = val\n",
    "                    cache[str(i) + \",\" + tag] = [tags[k], val]\n",
    "            \n",
    "            temp_list[j] = score\n",
    "        \n",
    "        viterbi_list = [x for x in temp_list]\n",
    "    \n",
    "    return cache, viterbi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, v = [], []\n",
    "for sentence in sentences_valid:\n",
    "    a, b = viterbi_decoding(trans_prob, em_prob, init_prob, sentence, tags)\n",
    "    c.append(a)\n",
    "    v.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_backward(tags, cache, viterbi_list):\n",
    "\n",
    "    num_states = len(tags)\n",
    "    n = len(cache) // num_states\n",
    "    best_sequence = []\n",
    "    best_sequence_breakdown = []\n",
    "    x = tags[np.argmax(np.asarray(viterbi_list))]\n",
    "    best_sequence.append(x)\n",
    "\n",
    "    for i in range(n, 0, -1):\n",
    "        val = cache[str(i) + ',' + x][1]\n",
    "        x = cache[str(i) + ',' + x][0]\n",
    "        best_sequence = [x] + best_sequence\n",
    "        best_sequence_breakdown =  [val] + best_sequence_breakdown\n",
    "    \n",
    "    return best_sequence, best_sequence_breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for viterbi decoding for validation: 0.95\n"
     ]
    }
   ],
   "source": [
    "viterbi_val_op = []\n",
    "best_seq_score = []\n",
    "for cache, viterbi_list in zip(c, v):\n",
    "\n",
    "    a, b = viterbi_backward(tags, cache, viterbi_list)\n",
    "    viterbi_val_op.append(a)\n",
    "    best_seq_score.append(b)\n",
    "\n",
    "print(\"Accuracy for viterbi decoding for validation: {:.2f}\".format(get_accuracy(viterbi_val_op, sentences_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Tags for Test data via Viterbi and saving them in viterbi.out file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, v = [], []\n",
    "\n",
    "for sentence in sentences_test:\n",
    "    a, b = viterbi_decoding(trans_prob, em_prob, init_prob, sentence, tags)\n",
    "    c.append(a)\n",
    "    v.append(b)\n",
    "\n",
    "viterbi_test_op= []\n",
    "for cache, viterbi_list in zip(c, v):\n",
    "\n",
    "    a, b = viterbi_backward(tags, cache, viterbi_list)\n",
    "    viterbi_test_op.append(a)\n",
    "\n",
    "output_file(sentences_test, viterbi_test_op, 'viterbi')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce271b99db1b250d22cac146c6eb9bdc2a3b9e3f607d82c95bc1468f582e9c43"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
